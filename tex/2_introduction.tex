\chapter{Introduction}

\section{Purpose}

The aim of this dissertation is to explore the use of  \gls{fp}, initially developed by Durrant and Kaban (2013)\cite{durrant2013sharp} as a \gls{lossfunction} in a \gls{cnn} for image classification. Previously, the concept of  \gls{fp} has been used in developing a robust form of logistic regression \cite{label_noise}. In practice, deep convolutional neural networks require vast amounts of data and computational resources to train. We ask whether we can leverage random projection of the data to alleviate these demands. We hypothesise that the use of \gls{fp} could improve classification accuracy, improve computational performance and could allow for \glsfirst{nn}s to be trained using less data. We test these hypotheses through the training and testing of convolutional neural networks. 
\bigskip

There are various methods of incorporating  \gls{fp} into a  \gls{nn}. In all cases, we wish to train the network by minimising the \gls{fp} and using it as a loss function. One method is to retrain the last layer\footnote{For example, described \url{https://www.tensorflow.org/tutorials/images/transfer_learning}{here}} using \gls{fp} as the loss while freezing the rest of the network. This is a method commonly used with high capacity \gls{dnn}s (described in \cite{transfer_learning}) \footnote{\href{https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html}{Pytorch tutorial here.}}. Another method would be to undergo \gls{representationlearning} on the data before beginning the training procedure, using \gls{fp} as the \gls{lossfunction}, thereby generating a representation that can be learned on more easily by a traditional loss function (such as \gls{softmax}) whilst the network then trained conventionally. A scheme used in this dissertation is alternating training of the network with  \gls{fp} as the \gls{loss} and a traditional \gls{lossfunction}. 
\bigskip

Part of the problem when investigating improvements to develop on \gls{nn} methodology is not just the complexity of the networks involved, but also the plethora of techniques now available. Hence here we focus on carefully testing the new \gls{lossfunction} with small, iterable data sets.
\bigskip

\section{Motivation}

A small  \gls{mse} is not a guarantee of a small $0/1$ loss - similar parts can have dissimilar labels. Points near the boundaries of classes are a key concern and likely to be misclassified.  \gls{mse} is a good aggregate measure of bulk of points. However, it is a poor measure of points near the decision boundary of classes. We need a smooth loss for fitting a classifier, not a discrete loss like the $0/1$ loss and a loss function that is superior to the  \gls{mse}. Backpropagation, a requirement for training \gls{dnn}s requires a smooth loss, not a discrete one. Ideally, we wish to optimise $0/1$ loss, but in practice, we can use \gls{fp} to assign more weight to those \enquote{errors} near the margin, which a loss function like the \gls{mse} will fail to do.
\bigskip

\section{Overview}

As the focus of this dissertation is to trial \gls{fp} as a \gls{lossfunction} for the purpose of image classification, we will restrict testing of this to \gls{cnn}s or simpler networks (such as \gls{mlp}s or \gls{feedforward} \gls{nn}s). 
\bigskip

 The \gls{fp} loss can be viewed as a smooth approximation of the $0/1$ loss. We want to enforce a \enquote{large margin} representation of the data. To implement this, we can use  \gls{nn}s "off the shelf", tweak them and measure results. However, \gls{fp} is a theoretical concept and finding an empirical measure is necessary if we wish to implement it in practice. 
 \bigskip

Once an empirical measure is found, we proceed to test this, using standard \gls{nn} training and testing techniques.
\bigskip