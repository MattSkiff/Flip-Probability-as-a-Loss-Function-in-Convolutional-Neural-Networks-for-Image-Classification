\section{Discussion}

It is feasible to implement \gls{fp} in a way that resembles the cosine loss, although this comes at the expense of increased computation burden and poorer training accuracies. Implementing \gls{fp} in a way that allows for \gls{gd} has still not been done.
\bigskip

Other work leveraging \gls{rp}s exists. For example, \gls{rp}s have been used in projection networks in tandem with the full \gls{nn} to efficiently compress down (reduce the number of parameters of) the full networks to lessen the memory footprint. This allows projection networks to run on devices that have resource constraints, such as smartwatches \cite{projection_net}. 
\bigskip

\section{Limitations}

Ideally, cross-validated error would have been used. Larger datasets, such as ImageNet could have also been used to test the new loss function in more complex context. There are various other optimisers that could have been trialled, such as Adagrad or Adadelta. There are a variety of methods of incorporating the loss function into the forward propagation, hence altering this process may have improved the optimisation of the network. Although the results were not successful, a systematic set of numeric tables may have allowed for greater inspection of the network outputs.