\begin{abstract}

\begin{spacing}{2}

Research in convolutional neural networks has exploded over the last eight years. However, much of the attention has been devoted to the architecture of these networks, and not to the internal optimisation process. We develop a novel implementation of a \gls{loss} function - the empirical \gls{flipprobability} loss. This is derived from the flipping probability (hereafter referred to as 'flip probability') developed by Durrant \& Kab\'an \cite{durrant2013sharp}. This is then tested in several canonical versions of convolutional neural networks to assess whether an increase in training speed or \gls{generalisationperformance} could be achieved. When used as a \gls{lossfunction}, flip probability can train networks of a reasonably high accuracy. However, the performance of flipping probability alone in terms of test accuracy and training was generally worse than when a standard \gls{lossfunction}, such as mean squared error, was used.

\end{spacing}

\end{abstract}

\begin{acknowledgements}

\bigskip

\begin{spacing}{2}

% ZAMINGO - is it 'was pivotal' or 'were pivotal'?
% removed - I think it overstates the results
% Check whether '.' after Dr
I would like to thank Dr Robert Durrant for his patience, wisdom and understanding in supervising this dissertation.
\bigskip

I am grateful for the support and kindness of my partner, Renata Sawyer.
\bigskip

\end{spacing}

\end{acknowledgements}