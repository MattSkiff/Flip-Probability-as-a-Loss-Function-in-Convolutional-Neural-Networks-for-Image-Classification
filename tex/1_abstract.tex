\begin{abstract}

\begin{spacing}{2}

Research in convolutional neural networks has exploded over the last eight years. However, much of the attention has been devoted to the architecture of these networks, and not to the internal optimisation process. We develop a novel implementation of a \gls{loss} function - the empirical \gls{flipprobability} loss. This is derived from the flipping probability (hereafter referred to as 'flip probability') developed by Durrant \& Kab\'an \cite{durrant2013sharp}. This is then tested in several canonical versions of convolution neural networks to assess whether an increase in training speed or \gls{generalisationperformance} could be achieved. While the \gls{lossfunction} can be used to train networks of a reasonably high accuracy, the performance, both in terms of test accuracy and training, is generally worse than when a standard \gls{lossfunction} and architecture is used. 

\end{spacing}

\end{abstract}

\begin{acknowledgements}

\bigskip

\begin{spacing}{2}

% ZAMINGO - is it 'was pivotal' or 'were pivotal'?
% removed - I think it overstates the results
% Check whether '.' after Dr
I would like to thank Dr Robert Durrant for his patience, wisdom and understanding in supervising this dissertation.
\bigskip

I am grateful for the support and kindness of my partner, Renata Sawyer.
\bigskip

\end{spacing}

\end{acknowledgements}