\begin{abstract}

\begin{spacing}{2}

We developed a novel implementation of a \gls{loss} function; the empirical \gls{flipprobability} loss, derived from the flip probability developed by Durrant \& Kab\'an \cite{durrant2013sharp} and tested it in several canonical versions of convolution neural networks to assess whether an increase in training speed or \gls{generalisationperformance} could be achieved. While the \gls{lossfunction} can be used to train networks of a reasonably high accuracy, the performance, both in terms of test accuracy and training, is generally worse than when a standard \gls{lossfunction} and architecture is used. 

\end{spacing}

\end{abstract}

\begin{acknowledgements}

\bigskip

\begin{spacing}{2}

% ZAMINGO - is it 'was pivotal' or 'were pivotal'?
% removed - I think it overstates the results
% Check whether '.' after Dr
I would like to thank Dr Robert Durrant for his patience, wisdom and understanding in supervising this dissertation.
\bigskip

I am grateful for the support and kindness of my partner, Renata Sawyer.
\bigskip

\end{spacing}

\end{acknowledgements}